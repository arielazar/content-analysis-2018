{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 4 - Word Embeddings Supplemental\n",
    "\n",
    "This notebook contains two additional uses for word embeddings\n",
    "\n",
    "For this notebook we will be using the following packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Special module written for this class\n",
    "#This provides access to data and to helper functions from previous weeks\n",
    "#Make sure you update it before starting this notebook\n",
    "import lucem_illud #pip install -U git+git://github.com/Computational-Content-Analysis-2018/lucem_illud.git\n",
    "\n",
    "#All these packages need to be installed from pip\n",
    "import gensim#For word2vec, etc\n",
    "import requests #For downloading our datasets\n",
    "import nltk #For stop words and stemmers\n",
    "import numpy as np #For arrays\n",
    "import pandas #Gives us DataFrames\n",
    "import matplotlib.pyplot as plt #For graphics\n",
    "import seaborn #Makes the graphics look nicer\n",
    "import sklearn.metrics.pairwise #For cosine similarity\n",
    "import sklearn.manifold #For T-SNE\n",
    "import sklearn.decomposition #For PCA\n",
    "import copy\n",
    "\n",
    "#gensim uses a couple of deprecated features\n",
    "#we can't do anything about them so lets ignore them \n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "\n",
    "#This 'magic' command makes the plots work better\n",
    "#in the notebook, don't use it outside of a notebook.\n",
    "#Also you can ignore the warning\n",
    "%matplotlib inline\n",
    "\n",
    "import os #For looking through files\n",
    "import os.path #For managing file paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Score Function\n",
    "\n",
    "The score function is a simple calculation developed by [Matt Taddy](https://arxiv.org/pdf/1504.07295.pdf) to calculate the likelihood that a given text would have been generated by a word-embedding model by summing the inner product between each pair of the text's word vectors. \n",
    "\n",
    "Here, we explore this using a model trained with millions of resumes from the CareerBuilder website (we can't share the private resumes...but we can share a model built with them :-):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "resume_model  = gensim.models.word2vec.Word2Vec.load('../data/resumeAll.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can examine the vacabularies of this model by building a word-index map:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab = resume_model.index2word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's just load the sample and take a look at it. The sentences in each job description are already tokenized and normalized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sampleDF = pandas.read_csv('../data/SampleJobAds.csv', index_col = False)\n",
    "#We need to convert the last couple columns from strings to lists\n",
    "sampleDF['tokenized_sents'] = sampleDF['tokenized_sents'].apply(lambda x: eval(x))\n",
    "sampleDF['normalized_sents'] = sampleDF['normalized_sents'].apply(lambda x: eval(x))\n",
    "sampleDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define a function to calculate the likelihood of each job description. The idea is borrowed from [Matt Taddy](https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/deepir.ipynb), who shows how a document can be characterized as the inner product of the distance between its words. In other words, this analysis will show which job ads are most likely to find an appropriate pool of workers in the resume bank that generated our word embedding.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def adprob(ad, model):\n",
    "    sen_scores = model.score(ad, len(ad))\n",
    "    ad_score = sen_scores.mean()\n",
    "    return ad_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's apply this function to every job description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sampleDF['likelihood'] = sampleDF['normalized_sents'].apply(lambda x: adprob(x, resume_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the top 5 job descriptions that have the highest likelihood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for ad in sampleDF.sort_values(by = 'likelihood', ascending = False)['jobDescription'][:5]:\n",
    "    print (ad + '\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the bottom 5 job descriptions that have the lowest likelihood to be matched by the resumes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for ad in sampleDF.sort_values(by = 'likelihood')['jobDescription'][:5]:\n",
    "    print (ad + '\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do the same for phrases corresponding to job skills."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "adprob([[\"python\", \"programming\"]], resume_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "adprob([[\"basic\", \"programming\"]], resume_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic programming appears to be more likely in this pool of resumes than python programming. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also do some simple statistics. Unfortunately, we don't have a large sample here. Nevertheless, let's first look at the mean likelihood score of each hiring organization. Some organizations will do well to hire on CareerBuilder...while others will not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sampleDF.groupby(\"hiringOrganization_organizationName\")[['likelihood']].mean().sort_values('likelihood', ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also look at the mean likelihood of each state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sampleDF.groupby(\"jobLocation_address_region\")[['likelihood']].mean().sort_values('likelihood', ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You would increase the sample size if you want to do a more serious study."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\">*Exercise 1a*</span>\n",
    "\n",
    "<span style=\"color:red\">**Do only 1a or 1b.** Construct cells immediately below this that calculate the scores for a small sample of documents from outside your corpus to identify which are *closest* to your corpus. Then calculate the scores for a few phrases or sentences to identify the ones most likely to have appeared in your corpus. Interrogate patterns associated with these document/phrase scores (e.g., which companies produced job ads most or least likely to find jobseekers in the resume corpus?) What do these patterns suggest about the boundaries of your corpus?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>asin</th>\n",
       "      <th>helpful</th>\n",
       "      <th>overall</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>reviewTime</th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>reviewerName</th>\n",
       "      <th>summary</th>\n",
       "      <th>unixReviewTime</th>\n",
       "      <th>tokenized_sents</th>\n",
       "      <th>normalized_sents</th>\n",
       "      <th>tokenized_words</th>\n",
       "      <th>normalized_words</th>\n",
       "      <th>uniqueID</th>\n",
       "      <th>TaggedReviews</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>616719923X</td>\n",
       "      <td>[2, 3]</td>\n",
       "      <td>5</td>\n",
       "      <td>Creamy white chocolate infused with Matcha gre...</td>\n",
       "      <td>07 5, 2013</td>\n",
       "      <td>A3FIVHUOGMUMPK</td>\n",
       "      <td>greenlife</td>\n",
       "      <td>So Delicious!!</td>\n",
       "      <td>1372982400</td>\n",
       "      <td>[['Creamy', 'white', 'chocolate', 'infused', '...</td>\n",
       "      <td>[['creamy', 'white', 'chocolate', 'infused', '...</td>\n",
       "      <td>['Creamy', 'white', 'chocolate', 'infused', 'w...</td>\n",
       "      <td>['creamy', 'white', 'chocolate', 'infused', 'm...</td>\n",
       "      <td>A3FIVHUOGMUMPK1372982400</td>\n",
       "      <td>LabeledSentence(['creamy', 'white', 'chocolate...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>616719923X</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>5</td>\n",
       "      <td>After hearing mixed opinions about these Kit K...</td>\n",
       "      <td>06 14, 2013</td>\n",
       "      <td>A27FSPAMTQF1J8</td>\n",
       "      <td>Japhyl</td>\n",
       "      <td>These are my favorite candies ever!</td>\n",
       "      <td>1371168000</td>\n",
       "      <td>[['After', 'hearing', 'mixed', 'opinions', 'ab...</td>\n",
       "      <td>[['hearing', 'mixed', 'opinions', 'kit', 'kats...</td>\n",
       "      <td>['After', 'hearing', 'mixed', 'opinions', 'abo...</td>\n",
       "      <td>['hearing', 'mixed', 'opinions', 'kit', 'kats'...</td>\n",
       "      <td>A27FSPAMTQF1J81371168000</td>\n",
       "      <td>LabeledSentence(['hearing', 'mixed', 'opinions...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>616719923X</td>\n",
       "      <td>[6, 8]</td>\n",
       "      <td>5</td>\n",
       "      <td>I ordered these in Summer so they of course ar...</td>\n",
       "      <td>10 2, 2013</td>\n",
       "      <td>A220GN2X2R47JE</td>\n",
       "      <td>Jeremy</td>\n",
       "      <td>Amazing!</td>\n",
       "      <td>1380672000</td>\n",
       "      <td>[['I', 'ordered', 'these', 'in', 'Summer', 'so...</td>\n",
       "      <td>[['ordered', 'summer', 'course', 'arrived', 'm...</td>\n",
       "      <td>['I', 'ordered', 'these', 'in', 'Summer', 'so'...</td>\n",
       "      <td>['ordered', 'summer', 'course', 'arrived', 'me...</td>\n",
       "      <td>A220GN2X2R47JE1380672000</td>\n",
       "      <td>LabeledSentence(['ordered', 'summer', 'course'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>616719923X</td>\n",
       "      <td>[2, 3]</td>\n",
       "      <td>5</td>\n",
       "      <td>These are definitely THE BEST candy bar out th...</td>\n",
       "      <td>05 26, 2013</td>\n",
       "      <td>A3C5Z05IKSSFB9</td>\n",
       "      <td>M. Magpoc \"maliasuperstar\"</td>\n",
       "      <td>I wish I could find these in a store instead o...</td>\n",
       "      <td>1369526400</td>\n",
       "      <td>[['These', 'are', 'definitely', 'THE', 'BEST',...</td>\n",
       "      <td>[['definitely', 'best', 'candy', 'bar'], ['wis...</td>\n",
       "      <td>['These', 'are', 'definitely', 'THE', 'BEST', ...</td>\n",
       "      <td>['definitely', 'best', 'candy', 'bar', 'wish',...</td>\n",
       "      <td>A3C5Z05IKSSFB91369526400</td>\n",
       "      <td>LabeledSentence(['definitely', 'best', 'candy'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>616719923X</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>5</td>\n",
       "      <td>Yes - this is one of the most expensive candie...</td>\n",
       "      <td>07 6, 2013</td>\n",
       "      <td>AHA6G4IMEMAJR</td>\n",
       "      <td>M. Zinn \"mczinn\"</td>\n",
       "      <td>Thank goodness they are expensive</td>\n",
       "      <td>1373068800</td>\n",
       "      <td>[['Yes', '-', 'this', 'is', 'one', 'of', 'the'...</td>\n",
       "      <td>[['yes', 'one', 'expensive', 'candies', 'aroun...</td>\n",
       "      <td>['Yes', '-', 'this', 'is', 'one', 'of', 'the',...</td>\n",
       "      <td>['yes', 'one', 'expensive', 'candies', 'around...</td>\n",
       "      <td>AHA6G4IMEMAJR1373068800</td>\n",
       "      <td>LabeledSentence(['yes', 'one', 'expensive', 'c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>616719923X</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>5</td>\n",
       "      <td>I love the green tea kitkat, taste so good, no...</td>\n",
       "      <td>06 8, 2013</td>\n",
       "      <td>A1Q2E3W9PRG313</td>\n",
       "      <td>Sabrina</td>\n",
       "      <td>it is good</td>\n",
       "      <td>1370649600</td>\n",
       "      <td>[['I', 'love', 'the', 'green', 'tea', 'kitkat'...</td>\n",
       "      <td>[['love', 'green', 'tea', 'kitkat', 'taste', '...</td>\n",
       "      <td>['I', 'love', 'the', 'green', 'tea', 'kitkat',...</td>\n",
       "      <td>['love', 'green', 'tea', 'kitkat', 'taste', 'g...</td>\n",
       "      <td>A1Q2E3W9PRG3131370649600</td>\n",
       "      <td>LabeledSentence(['love', 'green', 'tea', 'kitk...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>9742356831</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>5</td>\n",
       "      <td>This curry paste makes a delicious curry.  I j...</td>\n",
       "      <td>05 28, 2013</td>\n",
       "      <td>A23RYWDS884TUL</td>\n",
       "      <td>Another Freak</td>\n",
       "      <td>Delicious!</td>\n",
       "      <td>1369699200</td>\n",
       "      <td>[['This', 'curry', 'paste', 'makes', 'a', 'del...</td>\n",
       "      <td>[['curry', 'paste', 'makes', 'delicious', 'cur...</td>\n",
       "      <td>['This', 'curry', 'paste', 'makes', 'a', 'deli...</td>\n",
       "      <td>['curry', 'paste', 'makes', 'delicious', 'curr...</td>\n",
       "      <td>A23RYWDS884TUL1369699200</td>\n",
       "      <td>LabeledSentence(['curry', 'paste', 'makes', 'd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>9742356831</td>\n",
       "      <td>[1, 2]</td>\n",
       "      <td>5</td>\n",
       "      <td>I've purchased different curries in the grocer...</td>\n",
       "      <td>09 17, 2012</td>\n",
       "      <td>A945RBQWGZXCK</td>\n",
       "      <td>Cheryl</td>\n",
       "      <td>Great flavor</td>\n",
       "      <td>1347840000</td>\n",
       "      <td>[['I', \"'ve\", 'purchased', 'different', 'curri...</td>\n",
       "      <td>[['purchased', 'different', 'curries', 'grocer...</td>\n",
       "      <td>['I', \"'ve\", 'purchased', 'different', 'currie...</td>\n",
       "      <td>['purchased', 'different', 'curries', 'grocery...</td>\n",
       "      <td>A945RBQWGZXCK1347840000</td>\n",
       "      <td>LabeledSentence(['purchased', 'different', 'cu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>9742356831</td>\n",
       "      <td>[2, 2]</td>\n",
       "      <td>5</td>\n",
       "      <td>I love ethnic foods and to cook them. I recent...</td>\n",
       "      <td>08 3, 2013</td>\n",
       "      <td>A1TCSC0YWT82Q0</td>\n",
       "      <td>GinSing</td>\n",
       "      <td>OMG! What a treasure find!</td>\n",
       "      <td>1375488000</td>\n",
       "      <td>[['I', 'love', 'ethnic', 'foods', 'and', 'to',...</td>\n",
       "      <td>[['love', 'ethnic', 'foods', 'cook'], ['recent...</td>\n",
       "      <td>['I', 'love', 'ethnic', 'foods', 'and', 'to', ...</td>\n",
       "      <td>['love', 'ethnic', 'foods', 'cook', 'recently'...</td>\n",
       "      <td>A1TCSC0YWT82Q01375488000</td>\n",
       "      <td>LabeledSentence(['love', 'ethnic', 'foods', 'c...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          asin helpful  overall  \\\n",
       "7   616719923X  [2, 3]        5   \n",
       "8   616719923X  [0, 0]        5   \n",
       "10  616719923X  [6, 8]        5   \n",
       "11  616719923X  [2, 3]        5   \n",
       "12  616719923X  [0, 0]        5   \n",
       "13  616719923X  [0, 0]        5   \n",
       "16  9742356831  [0, 0]        5   \n",
       "17  9742356831  [1, 2]        5   \n",
       "18  9742356831  [2, 2]        5   \n",
       "\n",
       "                                           reviewText   reviewTime  \\\n",
       "7   Creamy white chocolate infused with Matcha gre...   07 5, 2013   \n",
       "8   After hearing mixed opinions about these Kit K...  06 14, 2013   \n",
       "10  I ordered these in Summer so they of course ar...   10 2, 2013   \n",
       "11  These are definitely THE BEST candy bar out th...  05 26, 2013   \n",
       "12  Yes - this is one of the most expensive candie...   07 6, 2013   \n",
       "13  I love the green tea kitkat, taste so good, no...   06 8, 2013   \n",
       "16  This curry paste makes a delicious curry.  I j...  05 28, 2013   \n",
       "17  I've purchased different curries in the grocer...  09 17, 2012   \n",
       "18  I love ethnic foods and to cook them. I recent...   08 3, 2013   \n",
       "\n",
       "        reviewerID                reviewerName  \\\n",
       "7   A3FIVHUOGMUMPK                   greenlife   \n",
       "8   A27FSPAMTQF1J8                      Japhyl   \n",
       "10  A220GN2X2R47JE                      Jeremy   \n",
       "11  A3C5Z05IKSSFB9  M. Magpoc \"maliasuperstar\"   \n",
       "12   AHA6G4IMEMAJR            M. Zinn \"mczinn\"   \n",
       "13  A1Q2E3W9PRG313                     Sabrina   \n",
       "16  A23RYWDS884TUL               Another Freak   \n",
       "17   A945RBQWGZXCK                      Cheryl   \n",
       "18  A1TCSC0YWT82Q0                     GinSing   \n",
       "\n",
       "                                              summary  unixReviewTime  \\\n",
       "7                                      So Delicious!!      1372982400   \n",
       "8                 These are my favorite candies ever!      1371168000   \n",
       "10                                           Amazing!      1380672000   \n",
       "11  I wish I could find these in a store instead o...      1369526400   \n",
       "12                  Thank goodness they are expensive      1373068800   \n",
       "13                                         it is good      1370649600   \n",
       "16                                         Delicious!      1369699200   \n",
       "17                                       Great flavor      1347840000   \n",
       "18                         OMG! What a treasure find!      1375488000   \n",
       "\n",
       "                                      tokenized_sents  \\\n",
       "7   [['Creamy', 'white', 'chocolate', 'infused', '...   \n",
       "8   [['After', 'hearing', 'mixed', 'opinions', 'ab...   \n",
       "10  [['I', 'ordered', 'these', 'in', 'Summer', 'so...   \n",
       "11  [['These', 'are', 'definitely', 'THE', 'BEST',...   \n",
       "12  [['Yes', '-', 'this', 'is', 'one', 'of', 'the'...   \n",
       "13  [['I', 'love', 'the', 'green', 'tea', 'kitkat'...   \n",
       "16  [['This', 'curry', 'paste', 'makes', 'a', 'del...   \n",
       "17  [['I', \"'ve\", 'purchased', 'different', 'curri...   \n",
       "18  [['I', 'love', 'ethnic', 'foods', 'and', 'to',...   \n",
       "\n",
       "                                     normalized_sents  \\\n",
       "7   [['creamy', 'white', 'chocolate', 'infused', '...   \n",
       "8   [['hearing', 'mixed', 'opinions', 'kit', 'kats...   \n",
       "10  [['ordered', 'summer', 'course', 'arrived', 'm...   \n",
       "11  [['definitely', 'best', 'candy', 'bar'], ['wis...   \n",
       "12  [['yes', 'one', 'expensive', 'candies', 'aroun...   \n",
       "13  [['love', 'green', 'tea', 'kitkat', 'taste', '...   \n",
       "16  [['curry', 'paste', 'makes', 'delicious', 'cur...   \n",
       "17  [['purchased', 'different', 'curries', 'grocer...   \n",
       "18  [['love', 'ethnic', 'foods', 'cook'], ['recent...   \n",
       "\n",
       "                                      tokenized_words  \\\n",
       "7   ['Creamy', 'white', 'chocolate', 'infused', 'w...   \n",
       "8   ['After', 'hearing', 'mixed', 'opinions', 'abo...   \n",
       "10  ['I', 'ordered', 'these', 'in', 'Summer', 'so'...   \n",
       "11  ['These', 'are', 'definitely', 'THE', 'BEST', ...   \n",
       "12  ['Yes', '-', 'this', 'is', 'one', 'of', 'the',...   \n",
       "13  ['I', 'love', 'the', 'green', 'tea', 'kitkat',...   \n",
       "16  ['This', 'curry', 'paste', 'makes', 'a', 'deli...   \n",
       "17  ['I', \"'ve\", 'purchased', 'different', 'currie...   \n",
       "18  ['I', 'love', 'ethnic', 'foods', 'and', 'to', ...   \n",
       "\n",
       "                                     normalized_words  \\\n",
       "7   ['creamy', 'white', 'chocolate', 'infused', 'm...   \n",
       "8   ['hearing', 'mixed', 'opinions', 'kit', 'kats'...   \n",
       "10  ['ordered', 'summer', 'course', 'arrived', 'me...   \n",
       "11  ['definitely', 'best', 'candy', 'bar', 'wish',...   \n",
       "12  ['yes', 'one', 'expensive', 'candies', 'around...   \n",
       "13  ['love', 'green', 'tea', 'kitkat', 'taste', 'g...   \n",
       "16  ['curry', 'paste', 'makes', 'delicious', 'curr...   \n",
       "17  ['purchased', 'different', 'curries', 'grocery...   \n",
       "18  ['love', 'ethnic', 'foods', 'cook', 'recently'...   \n",
       "\n",
       "                    uniqueID  \\\n",
       "7   A3FIVHUOGMUMPK1372982400   \n",
       "8   A27FSPAMTQF1J81371168000   \n",
       "10  A220GN2X2R47JE1380672000   \n",
       "11  A3C5Z05IKSSFB91369526400   \n",
       "12   AHA6G4IMEMAJR1373068800   \n",
       "13  A1Q2E3W9PRG3131370649600   \n",
       "16  A23RYWDS884TUL1369699200   \n",
       "17   A945RBQWGZXCK1347840000   \n",
       "18  A1TCSC0YWT82Q01375488000   \n",
       "\n",
       "                                        TaggedReviews  \n",
       "7   LabeledSentence(['creamy', 'white', 'chocolate...  \n",
       "8   LabeledSentence(['hearing', 'mixed', 'opinions...  \n",
       "10  LabeledSentence(['ordered', 'summer', 'course'...  \n",
       "11  LabeledSentence(['definitely', 'best', 'candy'...  \n",
       "12  LabeledSentence(['yes', 'one', 'expensive', 'c...  \n",
       "13  LabeledSentence(['love', 'green', 'tea', 'kitk...  \n",
       "16  LabeledSentence(['curry', 'paste', 'makes', 'd...  \n",
       "17  LabeledSentence(['purchased', 'different', 'cu...  \n",
       "18  LabeledSentence(['love', 'ethnic', 'foods', 'c...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "amazonRev5DF = pandas.read_csv('../4-Word-Embedding/amazonRev5DF.csv', index_col = 0)\n",
    "amazonRev5DF[1:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "amazonRev5W2V = gensim.models.word2vec.Word2Vec(amazonRev5DF['normalized_sents'].sum(),hs=1,negative=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>asin</th>\n",
       "      <th>helpful</th>\n",
       "      <th>overall</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>reviewTime</th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>reviewerName</th>\n",
       "      <th>summary</th>\n",
       "      <th>unixReviewTime</th>\n",
       "      <th>tokenized_sents</th>\n",
       "      <th>normalized_sents</th>\n",
       "      <th>tokenized_words</th>\n",
       "      <th>normalized_words</th>\n",
       "      <th>uniqueID</th>\n",
       "      <th>TaggedReviews</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>B00004S1C5</td>\n",
       "      <td>[8, 11]</td>\n",
       "      <td>1</td>\n",
       "      <td>This product is no where near natural / organi...</td>\n",
       "      <td>03 29, 2013</td>\n",
       "      <td>A14YSMLYLJEMET</td>\n",
       "      <td>Amazon Customer</td>\n",
       "      <td>Not natural/organic at all</td>\n",
       "      <td>1364515200</td>\n",
       "      <td>[['This', 'product', 'is', 'no', 'where', 'nea...</td>\n",
       "      <td>[['product', 'near', 'natural', 'wish', 'seen'...</td>\n",
       "      <td>['This', 'product', 'is', 'no', 'where', 'near...</td>\n",
       "      <td>['product', 'near', 'natural', 'wish', 'seen',...</td>\n",
       "      <td>A14YSMLYLJEMET1364515200</td>\n",
       "      <td>LabeledSentence(['product', 'near', 'natural',...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>B0000CCZYY</td>\n",
       "      <td>[1, 4]</td>\n",
       "      <td>1</td>\n",
       "      <td>Licorice is my favorite candy, and it promotes...</td>\n",
       "      <td>04 5, 2013</td>\n",
       "      <td>A3OH4OZFZGEH75</td>\n",
       "      <td>Amazon Customer</td>\n",
       "      <td>Not soft at all. Basically same as cheap licor...</td>\n",
       "      <td>1365120000</td>\n",
       "      <td>[['Licorice', 'is', 'my', 'favorite', 'candy',...</td>\n",
       "      <td>[['licorice', 'favorite', 'candy', 'promotes',...</td>\n",
       "      <td>['Licorice', 'is', 'my', 'favorite', 'candy', ...</td>\n",
       "      <td>['licorice', 'favorite', 'candy', 'promotes', ...</td>\n",
       "      <td>A3OH4OZFZGEH751365120000</td>\n",
       "      <td>LabeledSentence(['licorice', 'favorite', 'cand...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>B0000CCZYY</td>\n",
       "      <td>[6, 11]</td>\n",
       "      <td>1</td>\n",
       "      <td>This is an awesome product, natural, not a lot...</td>\n",
       "      <td>05 7, 2013</td>\n",
       "      <td>A2OUNVRPRWH0</td>\n",
       "      <td>The Kittie \"Kittie\"</td>\n",
       "      <td>Love this candy!</td>\n",
       "      <td>1367884800</td>\n",
       "      <td>[['This', 'is', 'an', 'awesome', 'product', ',...</td>\n",
       "      <td>[['awesome', 'product', 'natural', 'lot', 'ing...</td>\n",
       "      <td>['This', 'is', 'an', 'awesome', 'product', ','...</td>\n",
       "      <td>['awesome', 'product', 'natural', 'lot', 'ingr...</td>\n",
       "      <td>A2OUNVRPRWH01367884800</td>\n",
       "      <td>LabeledSentence(['awesome', 'product', 'natura...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>B0000CD06J</td>\n",
       "      <td>[0, 3]</td>\n",
       "      <td>1</td>\n",
       "      <td>As soon as I had a couple of sips, my eczema s...</td>\n",
       "      <td>03 6, 2013</td>\n",
       "      <td>AX04H2SPKO02S</td>\n",
       "      <td>J. Wang \"jyswang\"</td>\n",
       "      <td>NOT gluten free</td>\n",
       "      <td>1362528000</td>\n",
       "      <td>[['As', 'soon', 'as', 'I', 'had', 'a', 'couple...</td>\n",
       "      <td>[['soon', 'couple', 'sips', 'eczema', 'started...</td>\n",
       "      <td>['As', 'soon', 'as', 'I', 'had', 'a', 'couple'...</td>\n",
       "      <td>['soon', 'couple', 'sips', 'eczema', 'started'...</td>\n",
       "      <td>AX04H2SPKO02S1362528000</td>\n",
       "      <td>LabeledSentence(['soon', 'couple', 'sips', 'ec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>B0000CNU1X</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>1</td>\n",
       "      <td>unsure if I just got a bad batch or what...the...</td>\n",
       "      <td>01 23, 2013</td>\n",
       "      <td>A1M9L949MA66I3</td>\n",
       "      <td>orlandodawg</td>\n",
       "      <td>Not good</td>\n",
       "      <td>1358899200</td>\n",
       "      <td>[['unsure', 'if', 'I', 'just', 'got', 'a', 'ba...</td>\n",
       "      <td>[['unsure', 'got', 'bad', 'batch', 'flavor', '...</td>\n",
       "      <td>['unsure', 'if', 'I', 'just', 'got', 'a', 'bad...</td>\n",
       "      <td>['unsure', 'got', 'bad', 'batch', 'flavor', 'b...</td>\n",
       "      <td>A1M9L949MA66I31358899200</td>\n",
       "      <td>LabeledSentence(['unsure', 'got', 'bad', 'batc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217</th>\n",
       "      <td>B0000DGDMO</td>\n",
       "      <td>[1, 2]</td>\n",
       "      <td>1</td>\n",
       "      <td>Misleading.  The reason this is cheaper than t...</td>\n",
       "      <td>08 31, 2012</td>\n",
       "      <td>A30JPZ9TZ7I61U</td>\n",
       "      <td>Christopher Barrett \"Evil Corgi\"</td>\n",
       "      <td>Why is the picture showing the 24 pack?????</td>\n",
       "      <td>1346371200</td>\n",
       "      <td>[['Misleading', '.'], ['The', 'reason', 'this'...</td>\n",
       "      <td>[['misleading'], ['reason', 'cheaper', 'flavor...</td>\n",
       "      <td>['Misleading', '.', 'The', 'reason', 'this', '...</td>\n",
       "      <td>['misleading', 'reason', 'cheaper', 'flavors',...</td>\n",
       "      <td>A30JPZ9TZ7I61U1346371200</td>\n",
       "      <td>LabeledSentence(['misleading', 'reason', 'chea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>265</th>\n",
       "      <td>B0000DID5R</td>\n",
       "      <td>[6, 27]</td>\n",
       "      <td>1</td>\n",
       "      <td>Well, I guess I'm the fly in this reviewer oin...</td>\n",
       "      <td>01 14, 2007</td>\n",
       "      <td>A34PAZQ73SL163</td>\n",
       "      <td>Bernard Chapin \"Ora Et Labora!\"</td>\n",
       "      <td>The Only One I Avoid.</td>\n",
       "      <td>1168732800</td>\n",
       "      <td>[['Well', ',', 'I', 'guess', 'I', \"'m\", 'the',...</td>\n",
       "      <td>[['well', 'guess', 'fly', 'reviewer', 'ointmen...</td>\n",
       "      <td>['Well', ',', 'I', 'guess', 'I', \"'m\", 'the', ...</td>\n",
       "      <td>['well', 'guess', 'fly', 'reviewer', 'ointment...</td>\n",
       "      <td>A34PAZQ73SL1631168732800</td>\n",
       "      <td>LabeledSentence(['well', 'guess', 'fly', 'revi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270</th>\n",
       "      <td>B0000DID5R</td>\n",
       "      <td>[3, 4]</td>\n",
       "      <td>1</td>\n",
       "      <td>O.k., I'm going to offer a counterpoint to all...</td>\n",
       "      <td>11 11, 2012</td>\n",
       "      <td>A2MPW1R13SHA2S</td>\n",
       "      <td>Dangrenade</td>\n",
       "      <td>Unpleasant Heat, and No Flavor</td>\n",
       "      <td>1352592000</td>\n",
       "      <td>[['O.k.', ',', 'I', \"'m\", 'going', 'to', 'offe...</td>\n",
       "      <td>[['going', 'offer', 'counterpoint', 'positive'...</td>\n",
       "      <td>['O.k.', ',', 'I', \"'m\", 'going', 'to', 'offer...</td>\n",
       "      <td>['going', 'offer', 'counterpoint', 'positive',...</td>\n",
       "      <td>A2MPW1R13SHA2S1352592000</td>\n",
       "      <td>LabeledSentence(['going', 'offer', 'counterpoi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>290</th>\n",
       "      <td>B0000DID5R</td>\n",
       "      <td>[4, 24]</td>\n",
       "      <td>1</td>\n",
       "      <td>I just tried this sauce moments ago. Someone h...</td>\n",
       "      <td>03 1, 2011</td>\n",
       "      <td>A3FHWQ3H3ZT2YE</td>\n",
       "      <td>Patrice M. Christian \"Trixie.in.Dixie\"</td>\n",
       "      <td>Maybe my taste buds are different.</td>\n",
       "      <td>1298937600</td>\n",
       "      <td>[['I', 'just', 'tried', 'this', 'sauce', 'mome...</td>\n",
       "      <td>[['tried', 'sauce', 'moments', 'ago'], ['someo...</td>\n",
       "      <td>['I', 'just', 'tried', 'this', 'sauce', 'momen...</td>\n",
       "      <td>['tried', 'sauce', 'moments', 'ago', 'someone'...</td>\n",
       "      <td>A3FHWQ3H3ZT2YE1298937600</td>\n",
       "      <td>LabeledSentence(['tried', 'sauce', 'moments', ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           asin  helpful  overall  \\\n",
       "32   B00004S1C5  [8, 11]        1   \n",
       "75   B0000CCZYY   [1, 4]        1   \n",
       "82   B0000CCZYY  [6, 11]        1   \n",
       "85   B0000CD06J   [0, 3]        1   \n",
       "162  B0000CNU1X   [0, 0]        1   \n",
       "217  B0000DGDMO   [1, 2]        1   \n",
       "265  B0000DID5R  [6, 27]        1   \n",
       "270  B0000DID5R   [3, 4]        1   \n",
       "290  B0000DID5R  [4, 24]        1   \n",
       "\n",
       "                                            reviewText   reviewTime  \\\n",
       "32   This product is no where near natural / organi...  03 29, 2013   \n",
       "75   Licorice is my favorite candy, and it promotes...   04 5, 2013   \n",
       "82   This is an awesome product, natural, not a lot...   05 7, 2013   \n",
       "85   As soon as I had a couple of sips, my eczema s...   03 6, 2013   \n",
       "162  unsure if I just got a bad batch or what...the...  01 23, 2013   \n",
       "217  Misleading.  The reason this is cheaper than t...  08 31, 2012   \n",
       "265  Well, I guess I'm the fly in this reviewer oin...  01 14, 2007   \n",
       "270  O.k., I'm going to offer a counterpoint to all...  11 11, 2012   \n",
       "290  I just tried this sauce moments ago. Someone h...   03 1, 2011   \n",
       "\n",
       "         reviewerID                            reviewerName  \\\n",
       "32   A14YSMLYLJEMET                         Amazon Customer   \n",
       "75   A3OH4OZFZGEH75                         Amazon Customer   \n",
       "82     A2OUNVRPRWH0                     The Kittie \"Kittie\"   \n",
       "85    AX04H2SPKO02S                       J. Wang \"jyswang\"   \n",
       "162  A1M9L949MA66I3                             orlandodawg   \n",
       "217  A30JPZ9TZ7I61U        Christopher Barrett \"Evil Corgi\"   \n",
       "265  A34PAZQ73SL163         Bernard Chapin \"Ora Et Labora!\"   \n",
       "270  A2MPW1R13SHA2S                              Dangrenade   \n",
       "290  A3FHWQ3H3ZT2YE  Patrice M. Christian \"Trixie.in.Dixie\"   \n",
       "\n",
       "                                               summary  unixReviewTime  \\\n",
       "32                          Not natural/organic at all      1364515200   \n",
       "75   Not soft at all. Basically same as cheap licor...      1365120000   \n",
       "82                                    Love this candy!      1367884800   \n",
       "85                                     NOT gluten free      1362528000   \n",
       "162                                           Not good      1358899200   \n",
       "217        Why is the picture showing the 24 pack?????      1346371200   \n",
       "265                              The Only One I Avoid.      1168732800   \n",
       "270                     Unpleasant Heat, and No Flavor      1352592000   \n",
       "290                 Maybe my taste buds are different.      1298937600   \n",
       "\n",
       "                                       tokenized_sents  \\\n",
       "32   [['This', 'product', 'is', 'no', 'where', 'nea...   \n",
       "75   [['Licorice', 'is', 'my', 'favorite', 'candy',...   \n",
       "82   [['This', 'is', 'an', 'awesome', 'product', ',...   \n",
       "85   [['As', 'soon', 'as', 'I', 'had', 'a', 'couple...   \n",
       "162  [['unsure', 'if', 'I', 'just', 'got', 'a', 'ba...   \n",
       "217  [['Misleading', '.'], ['The', 'reason', 'this'...   \n",
       "265  [['Well', ',', 'I', 'guess', 'I', \"'m\", 'the',...   \n",
       "270  [['O.k.', ',', 'I', \"'m\", 'going', 'to', 'offe...   \n",
       "290  [['I', 'just', 'tried', 'this', 'sauce', 'mome...   \n",
       "\n",
       "                                      normalized_sents  \\\n",
       "32   [['product', 'near', 'natural', 'wish', 'seen'...   \n",
       "75   [['licorice', 'favorite', 'candy', 'promotes',...   \n",
       "82   [['awesome', 'product', 'natural', 'lot', 'ing...   \n",
       "85   [['soon', 'couple', 'sips', 'eczema', 'started...   \n",
       "162  [['unsure', 'got', 'bad', 'batch', 'flavor', '...   \n",
       "217  [['misleading'], ['reason', 'cheaper', 'flavor...   \n",
       "265  [['well', 'guess', 'fly', 'reviewer', 'ointmen...   \n",
       "270  [['going', 'offer', 'counterpoint', 'positive'...   \n",
       "290  [['tried', 'sauce', 'moments', 'ago'], ['someo...   \n",
       "\n",
       "                                       tokenized_words  \\\n",
       "32   ['This', 'product', 'is', 'no', 'where', 'near...   \n",
       "75   ['Licorice', 'is', 'my', 'favorite', 'candy', ...   \n",
       "82   ['This', 'is', 'an', 'awesome', 'product', ','...   \n",
       "85   ['As', 'soon', 'as', 'I', 'had', 'a', 'couple'...   \n",
       "162  ['unsure', 'if', 'I', 'just', 'got', 'a', 'bad...   \n",
       "217  ['Misleading', '.', 'The', 'reason', 'this', '...   \n",
       "265  ['Well', ',', 'I', 'guess', 'I', \"'m\", 'the', ...   \n",
       "270  ['O.k.', ',', 'I', \"'m\", 'going', 'to', 'offer...   \n",
       "290  ['I', 'just', 'tried', 'this', 'sauce', 'momen...   \n",
       "\n",
       "                                      normalized_words  \\\n",
       "32   ['product', 'near', 'natural', 'wish', 'seen',...   \n",
       "75   ['licorice', 'favorite', 'candy', 'promotes', ...   \n",
       "82   ['awesome', 'product', 'natural', 'lot', 'ingr...   \n",
       "85   ['soon', 'couple', 'sips', 'eczema', 'started'...   \n",
       "162  ['unsure', 'got', 'bad', 'batch', 'flavor', 'b...   \n",
       "217  ['misleading', 'reason', 'cheaper', 'flavors',...   \n",
       "265  ['well', 'guess', 'fly', 'reviewer', 'ointment...   \n",
       "270  ['going', 'offer', 'counterpoint', 'positive',...   \n",
       "290  ['tried', 'sauce', 'moments', 'ago', 'someone'...   \n",
       "\n",
       "                     uniqueID  \\\n",
       "32   A14YSMLYLJEMET1364515200   \n",
       "75   A3OH4OZFZGEH751365120000   \n",
       "82     A2OUNVRPRWH01367884800   \n",
       "85    AX04H2SPKO02S1362528000   \n",
       "162  A1M9L949MA66I31358899200   \n",
       "217  A30JPZ9TZ7I61U1346371200   \n",
       "265  A34PAZQ73SL1631168732800   \n",
       "270  A2MPW1R13SHA2S1352592000   \n",
       "290  A3FHWQ3H3ZT2YE1298937600   \n",
       "\n",
       "                                         TaggedReviews  \n",
       "32   LabeledSentence(['product', 'near', 'natural',...  \n",
       "75   LabeledSentence(['licorice', 'favorite', 'cand...  \n",
       "82   LabeledSentence(['awesome', 'product', 'natura...  \n",
       "85   LabeledSentence(['soon', 'couple', 'sips', 'ec...  \n",
       "162  LabeledSentence(['unsure', 'got', 'bad', 'batc...  \n",
       "217  LabeledSentence(['misleading', 'reason', 'chea...  \n",
       "265  LabeledSentence(['well', 'guess', 'fly', 'revi...  \n",
       "270  LabeledSentence(['going', 'offer', 'counterpoi...  \n",
       "290  LabeledSentence(['tried', 'sauce', 'moments', ...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "amazonRev1DF = pandas.read_csv('../4-Word-Embedding/amazonRev1DF.csv', index_col = 0)\n",
    "amazonRev1DF[1:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def adprob(ad, model):\n",
    "    sen_scores = model.score(ad, len(ad))\n",
    "    ad_score = sen_scores.mean()\n",
    "    return ad_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "amazonRev1DF['likelihood'] = amazonRev1DF['normalized_sents'].apply(lambda x: adprob(x, amazonRev5W2V))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i only got one little packet of these, not the 30 count that is says.it tasted like medicine and made me very jittery.it also made my heart race.i didn't like it\n",
      "\n",
      "\n",
      "The tea I received was old, old, old, the flavor was gone, and none of it had any taste to it. The tins are very cute, but unless you're into collecting cute little tins, I would skip this one.\n",
      "\n",
      "\n",
      "This item tastes odd and I find the experation date of less than two months from the date of receipt is too short. AT least I was able to return it even tho I loose half the price of this order just to return it but better safe than sorry. From now on I will get my mayo at my local store even if I can't find the 64 oz. jar.\n",
      "\n",
      "\n",
      "I have found Stash teas to be very hit or miss.  I love Moroccan Mint teas in general and decided to give Stash's a try.  Upon opening the packet there was no mint scent at all.  That was a red alert as mint has a crisp and strong scent for a very long time (I found a box of mint tea in my pantry that was two years past date and when opened still had strong clean mint scent).I've made tea with the Stash Moroccan Mint Green four times now and it not only doesn't have the mint flavor, it doesn't have much of a green tea flavor either.I tried using hotter water, steeping a long time and nothing makes it better.using it as a filler tea now with various others.\n",
      "\n",
      "\n",
      "they decided to ship both my order in ONE HUGE box of 24 cans instead of 12 green beans in one box, 12 corn in another and i was barely able to get this inside my house.  then every single can of green beans was dented and 3 cans of corn were dented\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for ad in amazonRev1DF.sort_values(by = 'likelihood', ascending = False)['reviewText'][:5]:\n",
    "    print (ad + '\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "DOES NOT WORK for:-Oil Pulling-Skin Nutrition-Cooking\n",
      "didn't like it\n",
      "Yuk, just yuk.\n",
      "so nasty... the taste was like having herpes.. ewwwwwwwwwwwwwwwwwwwwwwwwhahahah, lOLnasty, toilet, garbage, this product should not even exist... it's so nasty.\n",
      "Awful tasting....\n",
      "These are probably sensible but taste awful, you can't even tell which fruit you are eating, yuckkkkkkkkkkkkkkkk.\n",
      "my dog loved them. I can not stand the, this is not jerky at all.\n",
      "Not at all what I expected - no mellow flavor just cardboard.  Very disappointed.  Won't buy it again.\n",
      "I am all about adventure but this was worse than the sinkig of the titanic...What ever flavor this is supposed to be it isn't, it is just funkyyyyyy....\n",
      "This is not what I was looking for at all. I am looking for simple microwave products. I will not order them again.\n",
      "I served this for Thanksgiving and I and my guests did not like any of the flavors.  It was too preservative tasting.\n",
      "This French Vanilla Cappuccino was just plain awful. It lacked any distinctive flavor. I will not be ordering this again.\n",
      "Yuck. Myself or my 4yr old didn't like this.\n",
      "Very salty. I'm not buying these again. Better ramin can be bought for $1.00 at the market. Sorry , but just not a quality product.\n",
      "Very little flavor, & overwhelmingly too sweet. This was a big disappointment. I love coffee flavors, & especially pumpkin. Back to Starbucks I guess.\n",
      "just yakisoba FAVOR. not yakisoba.it is not that bad but kind of greasy.just yakisoba FAVOR. not yakisoba.it is not that bad but kind of greasy.\n"
     ]
    }
   ],
   "source": [
    "for ad in amazonRev1DF.sort_values(by = 'likelihood')['reviewText'][:20]:\n",
    "    print (ad )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adprob([[\"tasty\"]], amazonRev5W2V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Word2Vec' object has no attribute 'index2word'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-cf39653b24b4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mamazonRev5W2V\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex2word\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'Word2Vec' object has no attribute 'index2word'"
     ]
    }
   ],
   "source": [
    "amazonRev5W2V.index2word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linguistic Change"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is code that aligns the dimensions of multiple embeddings arrayed over time or some other dimension and allow identification of semantic chanage as the word vectors change their loadings for focal words. This code comes from the approach piloted at Stanford by William Hamilton, Daniel Jurafsky and Jure Lescovec [here](https://arxiv.org/pdf/1605.09096.pdf). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_syn0norm(model):\n",
    "    \"\"\"since syn0norm is now depricated\"\"\"\n",
    "    return (model.wv.syn0 / np.sqrt((model.wv.syn0 ** 2).sum(-1))[..., np.newaxis]).astype(np.float32)\n",
    "\n",
    "def smart_procrustes_align_gensim(base_embed, other_embed, words=None):\n",
    "    \"\"\"Procrustes align two gensim word2vec models (to allow for comparison between same word across models).\n",
    "    Code ported from HistWords <https://github.com/williamleif/histwords> by William Hamilton <wleif@stanford.edu>.\n",
    "    (With help from William. Thank you!)\n",
    "    First, intersect the vocabularies (see `intersection_align_gensim` documentation).\n",
    "    Then do the alignment on the other_embed model.\n",
    "    Replace the other_embed model's syn0 and syn0norm numpy matrices with the aligned version.\n",
    "    Return other_embed.\n",
    "    If `words` is set, intersect the two models' vocabulary with the vocabulary in words (see `intersection_align_gensim` documentation).\n",
    "    \"\"\"\n",
    "    base_embed = copy.copy(base_embed)\n",
    "    other_embed = copy.copy(other_embed)\n",
    "    # make sure vocabulary and indices are aligned\n",
    "    in_base_embed, in_other_embed = intersection_align_gensim(base_embed, other_embed, words=words)\n",
    "\n",
    "    # get the embedding matrices\n",
    "    base_vecs = calc_syn0norm(in_base_embed)\n",
    "    other_vecs = calc_syn0norm(in_other_embed)\n",
    "\n",
    "    # just a matrix dot product with numpy\n",
    "    m = other_vecs.T.dot(base_vecs) \n",
    "    # SVD method from numpy\n",
    "    u, _, v = np.linalg.svd(m)\n",
    "    # another matrix operation\n",
    "    ortho = u.dot(v) \n",
    "    # Replace original array with modified one\n",
    "    # i.e. multiplying the embedding matrix (syn0norm)by \"ortho\"\n",
    "    other_embed.wv.syn0norm = other_embed.wv.syn0 = (calc_syn0norm(other_embed)).dot(ortho)\n",
    "    return other_embed\n",
    "    \n",
    "def intersection_align_gensim(m1,m2, words=None):\n",
    "    \"\"\"\n",
    "    Intersect two gensim word2vec models, m1 and m2.\n",
    "    Only the shared vocabulary between them is kept.\n",
    "    If 'words' is set (as list or set), then the vocabulary is intersected with this list as well.\n",
    "    Indices are re-organized from 0..N in order of descending frequency (=sum of counts from both m1 and m2).\n",
    "    These indices correspond to the new syn0 and syn0norm objects in both gensim models:\n",
    "        -- so that Row 0 of m1.syn0 will be for the same word as Row 0 of m2.syn0\n",
    "        -- you can find the index of any word on the .index2word list: model.index2word.index(word) => 2\n",
    "    The .vocab dictionary is also updated for each model, preserving the count but updating the index.\n",
    "    \"\"\"\n",
    "\n",
    "    # Get the vocab for each model\n",
    "    vocab_m1 = set(m1.wv.vocab.keys())\n",
    "    vocab_m2 = set(m2.wv.vocab.keys())\n",
    "\n",
    "    # Find the common vocabulary\n",
    "    common_vocab = vocab_m1&vocab_m2\n",
    "    if words: common_vocab&=set(words)\n",
    "\n",
    "    # If no alignment necessary because vocab is identical...\n",
    "    if not vocab_m1-common_vocab and not vocab_m2-common_vocab:\n",
    "        return (m1,m2)\n",
    "\n",
    "    # Otherwise sort by frequency (summed for both)\n",
    "    common_vocab = list(common_vocab)\n",
    "    common_vocab.sort(key=lambda w: m1.wv.vocab[w].count + m2.wv.vocab[w].count,reverse=True)\n",
    "\n",
    "    # Then for each model...\n",
    "    for m in [m1,m2]:\n",
    "        # Replace old syn0norm array with new one (with common vocab)\n",
    "        indices = [m.wv.vocab[w].index for w in common_vocab]\n",
    "        old_arr = calc_syn0norm(m)\n",
    "        new_arr = np.array([old_arr[index] for index in indices])\n",
    "        m.wv.syn0norm = m.wv.syn0 = new_arr\n",
    "\n",
    "        # Replace old vocab dictionary with new one (with common vocab)\n",
    "        # and old index2word with new one\n",
    "        m.index2word = common_vocab\n",
    "        old_vocab = m.wv.vocab\n",
    "        new_vocab = {}\n",
    "        for new_index,word in enumerate(common_vocab):\n",
    "            old_vocab_obj=old_vocab[word]\n",
    "            new_vocab[word] = gensim.models.word2vec.Vocab(index=new_index, count=old_vocab_obj.count)\n",
    "        m.wv.vocab = new_vocab\n",
    "\n",
    "    return (m1,m2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to explore this, let's get some data that follows a time trend. We'll look at conference proceedings from the American Society for Clinical Oncologists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ascoDF = pandas.read_csv(\"../data/ASCO_abstracts.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare for wor2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ascoDF['tokenized_sents'] = ascoDF['Body'].apply(lambda x: [nltk.word_tokenize(s) for s in nltk.sent_tokenize(x)])\n",
    "ascoDF['normalized_sents'] = ascoDF['tokenized_sents'].apply(lambda x: [lucem_illud.normalizeTokens(s, stopwordLst = lucem_illud.stop_words_basic) for s in x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be creating many embeddings so we have created this function to do most of the work. It creates two collections of embeddings, one the original and one the aligned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compareModels(df, category, sort = True):\n",
    "    \"\"\"If you are using time as your category sorting is important\"\"\"\n",
    "    embeddings_raw = {}\n",
    "    cats = sorted(set(df[category]))\n",
    "    for cat in cats:\n",
    "        #This can take a while\n",
    "        print(\"Embedding {}\".format(cat), end = '\\r')\n",
    "        subsetDF = df[df[category] == cat]\n",
    "        #You might want to change the W2V parameters\n",
    "        embeddings_raw[cat] = gensim.models.word2vec.Word2Vec(subsetDF['normalized_sents'].sum())\n",
    "    #These are much quicker\n",
    "    embeddings_aligned = {}\n",
    "    for catOuter in cats:\n",
    "        embeddings_aligned[catOuter] = [embeddings_raw[catOuter]]\n",
    "        for catInner in cats:\n",
    "            embeddings_aligned[catOuter].append(smart_procrustes_align_gensim(embeddings_aligned[catOuter][-1], embeddings_raw[catInner]))\n",
    "    return embeddings_raw, embeddings_aligned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we generate the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rawEmbeddings, comparedEmbeddings = compareModels(ascoDF, 'Year')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to compare them across all permutions so we will define another function to help, we will be using 1 - cosine similarity as that gives a more intitive range of 0-2 with low values meaning little change and high meaning lots of change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getDivergenceDF(word, embeddingsDict):\n",
    "    dists = []\n",
    "    cats = sorted(set(embeddingsDict.keys()))\n",
    "    dists = {}\n",
    "    for cat in cats:\n",
    "        dists[cat] = []\n",
    "        for embed in embeddingsDict[cat][1:]:\n",
    "            dists[cat].append(np.abs(1 - sklearn.metrics.pairwise.cosine_similarity(np.expand_dims(embeddingsDict[cat][0][word], axis = 0),\n",
    "                                                                             np.expand_dims(embed[word], axis = 0))[0,0]))\n",
    "    return pandas.DataFrame(dists, index = cats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets look at a couple words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "targetWord = 'breast'\n",
    "\n",
    "pltDF = getDivergenceDF(targetWord, comparedEmbeddings)\n",
    "fig, ax = plt.subplots(figsize = (10, 7))\n",
    "seaborn.heatmap(pltDF, ax = ax, annot = False) #set annot True for a lot more information\n",
    "ax.set_xlabel(\"Starting year\")\n",
    "ax.set_ylabel(\"Final year\")\n",
    "ax.set_ylabel(\"Final year\")\n",
    "ax.set_title(\"Yearly linguistic change for: '{}'\".format(targetWord))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "targetWord = 'triple'\n",
    "\n",
    "pltDF = getDivergenceDF(targetWord, comparedEmbeddings)\n",
    "fig, ax = plt.subplots(figsize = (10, 7))\n",
    "seaborn.heatmap(pltDF, ax = ax, annot = False) #set annot True for a lot more information\n",
    "ax.set_xlabel(\"Starting year\")\n",
    "ax.set_ylabel(\"Final year\")\n",
    "ax.set_ylabel(\"Final year\")\n",
    "ax.set_title(\"Yearly linguistic change for: '{}'\".format(targetWord))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also ask which words changed the most"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def findDiverence(word, embeddingsDict):\n",
    "    cats = sorted(set(embeddingsDict.keys()))\n",
    "    \n",
    "    dists = []\n",
    "    for embed in embeddingsDict[cats[0]][1:]:\n",
    "        dists.append(1 - sklearn.metrics.pairwise.cosine_similarity(np.expand_dims(embeddingsDict[cats[0]][0][word], axis = 0), np.expand_dims(embed[word], axis = 0))[0,0])\n",
    "    return sum(dists)\n",
    "\n",
    "def findMostDivergent(embeddingsDict):\n",
    "    words = []\n",
    "    for embeds in embeddingsDict.values():\n",
    "        for embed in embeds:\n",
    "            words += list(embed.wv.vocab.keys())\n",
    "    words = set(words)\n",
    "    print(\"Found {} words to compare\".format(len(words)))\n",
    "    return sorted([(w, findDiverence(w, embeddingsDict)) for w in words], key = lambda x: x[1], reverse=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wordDivergences = findMostDivergent(comparedEmbeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most divergent words are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wordDivergences[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the least"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wordDivergences[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "targetWord = wordDivergences[0][0]\n",
    "\n",
    "pltDF = getDivergenceDF(targetWord, comparedEmbeddings)\n",
    "fig, ax = plt.subplots(figsize = (10, 7))\n",
    "seaborn.heatmap(pltDF, ax = ax, annot = False) #set annot True for a lot more information\n",
    "ax.set_xlabel(\"Starting year\")\n",
    "ax.set_ylabel(\"Final year\")\n",
    "ax.set_ylabel(\"Final year\")\n",
    "ax.set_title(\"Yearly linguistic change for: '{}'\".format(targetWord))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "targetWord = wordDivergences[-1][0]\n",
    "\n",
    "pltDF = getDivergenceDF(targetWord, comparedEmbeddings)\n",
    "fig, ax = plt.subplots(figsize = (10, 7))\n",
    "seaborn.heatmap(pltDF, ax = ax, annot = False) #set annot True for a lot more information\n",
    "ax.set_xlabel(\"Starting year\")\n",
    "ax.set_ylabel(\"Final year\")\n",
    "ax.set_ylabel(\"Final year\")\n",
    "ax.set_title(\"Yearly linguistic change for: '{}'\".format(targetWord))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\">*Exercise 1b*</span>\n",
    "\n",
    "<span style=\"color:red\">**Do only 3a or 3b.** Construct cells immediately below this that align word embeddings over time. Interrogate the spaces that result and ask which words change most of the whole period. What does this reveal about the social game underlying your space?"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
